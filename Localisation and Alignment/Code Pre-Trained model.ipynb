{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["TUNF8Lu5_X4H","S2FeRxmJ_hwN","xFJhtxEo_9VL","XtyF6i6NDEgn"],"authorship_tag":"ABX9TyMQZLsauZldWwCTp1uiloev"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["import os \n","import torch \n","import albumentations as A\n","import cv2\n","import numpy as np\n","from albumentations.pytorch import ToTensorV2\n","import glob as glob\n","from xml.etree import ElementTree as et\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","import torch\n","import matplotlib.pyplot as plt\n","import time\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import torch.nn as nn\n","import sklearn\n","from torchvision import ops"],"metadata":{"id":"gLo7a3zx-YWA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvbd1Ahb97RI","executionInfo":{"status":"ok","timestamp":1678474217787,"user_tz":-60,"elapsed":25692,"user":{"displayName":"TV","userId":"10344795622597267643"}},"outputId":"f87240ed-690a-4fa9-d20c-829fcdf9e2aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/DeepVis/Kontron')\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"OPr_tSNK-QNM","executionInfo":{"status":"ok","timestamp":1677347416876,"user_tz":-60,"elapsed":1195,"user":{"displayName":"TV","userId":"10344795622597267643"}},"outputId":"0bfbf16f-1d77-4494-c8cb-8b6399032aae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/DeepVis/Kontron'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"TUNF8Lu5_X4H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUPxiwCl9yhV"},"outputs":[],"source":["# Batch, Size, Epoch, Nettype, optimizer\n","\n","\n","BATCH_SIZE = 4\n","RESIZE_TO = 512\n","NUM_EPOCHS = 50\n","\n","DEVICE = torch.device(\"cuda\")\n","\n","TRAIN_DIR = \"/content/drive/MyDrive/DeepVis/Kontron/Newdata/train_test\"\n","VALID_DIR = \"/content/drive/MyDrive/DeepVis/Kontron/Newdata/val_test\"\n","\n","CLASSES = ['background', 'CamFront']\n","NUM_CLASSES = 2\n","\n","VISUALIZE_TRANSFORMED_IMAGES = True\n","\n","OUT_DIR = '/content/drive/MyDrive/DeepVis/Kontron/Newdata/output_test'\n","\n","SAVE_PLOTS_EPOCH = 100\n","SAVE_MODEL_EPOCH = 100"]},{"cell_type":"markdown","source":["# Transformations and loss class"],"metadata":{"id":"S2FeRxmJ_hwN"}},{"cell_type":"code","source":["# class to keep track of the Loss\n","class Averager:\n","    def __init__(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","        \n","    def send(self, value):\n","        self.current_total += value\n","        self.iterations += 1\n","    \n","    @property\n","    def value(self):\n","        if self.iterations == 0:\n","            return 0\n","        else:\n","            return 1.0 * self.current_total / self.iterations\n","    \n","    def reset(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","# training transformations \n","def get_train_transform():\n","    return A.Compose([\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], \n","                    max_pixel_value=255.0, p=1.0),\n","\n","        A.Rotate(limit=10, p=0.2),\n","        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.3),\n","\n","       \n","        ToTensorV2(p=1.0),\n","    ], bbox_params={\n","        'format': 'pascal_voc',\n","        'label_fields': ['labels']\n","    })\n","\n","# validation transformations\n","def get_valid_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0),\n","    ], bbox_params={\n","        'format': 'pascal_voc', \n","        'label_fields': ['labels']\n","    })\n","\n","# print transformed image to the screen\n","def show_transformed_image(train_loader):\n","    if len(train_loader) > 0:\n","        for i in range(3):\n","            images, targets = next(iter(train_loader))\n","            images = list(image.to(DEVICE) for image in images)\n","            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n","            sample = images[i].permute(1, 2, 0).cpu().numpy()\n","            for box in boxes:\n","                cv2.rectangle(sample,\n","                            (box[0], box[1]),\n","                            (box[2], box[3]),\n","                            (0, 0, 0), 0)\n","            plt.imshow(sample)\n","            plt.show()\n","            cv2.waitKey(0)\n","            cv2.destroyAllWindows()\n","    "],"metadata":{"id":"gJigflzl_kQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Datasets"],"metadata":{"id":"xFJhtxEo_9VL"}},{"cell_type":"code","source":["class KDataset(Dataset):\n","    def __init__(self, dir_path, width, height, classes, transforms=None):\n","        self.transforms = transforms\n","        self.dir_path = dir_path\n","        self.height = height\n","        self.width = width\n","        self.classes = classes\n","        \n","        # get image paths\n","        self.image_paths = glob.glob(f\"{self.dir_path}/*.jpg\")\n","        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n","        self.all_images = sorted(self.all_images)\n","        \n","    def __getitem__(self, idx):\n","            # image name \n","            image_name = self.all_images[idx]\n","            image_path = os.path.join(self.dir_path, image_name)\n","\n","            # read image \n","            image = cv2.imread(image_path)\n","            # convert color format and resize \n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","            image_resized = image\n","            image_resized = cv2.resize(image, (self.width, self.height))\n","            \n","          \n","           # get xml file for the image to claculate IoU later on \n","            annot_filename = image_name[:-4] + '.xml'\n","            annot_file_path = os.path.join(self.dir_path, annot_filename)\n","            \n","            boxes = []\n","            labels = []\n","            tree = et.parse(annot_file_path)\n","            root = tree.getroot()\n","            \n","            image_width = image.shape[1]\n","            image_height = image.shape[0]\n","            \n","            for member in root.findall('object'):\n","                \n","                # get the coordinates of the bounding box\n","                if member.find('name').text == 'CamFront':\n","                    labels.append(self.classes.index(member.find('name').text))\n","                    xmin = int(member.find('bndbox').find('xmin').text)\n","                    xmax = int(member.find('bndbox').find('xmax').text)\n","                    ymin = int(member.find('bndbox').find('ymin').text)\n","                    ymax = int(member.find('bndbox').find('ymax').text)\n","                    \n","                \n","                # resize bounding box \n","                    xmin_final = (xmin/image_width)*self.width\n","                    xmax_final = (xmax/image_width)*self.width\n","                    ymin_final = (ymin/image_height)*self.height\n","                    yamx_final = (ymax/image_height)*self.height\n","                    \n","                    boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n","                  # boubding box to tensor \n","                    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","                  # calculate the area of the bounding boxes\n","                    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","                    labels = torch.as_tensor(labels, dtype=torch.int64)\n","                  # make a target dict\n","                    target = {}\n","                    target[\"boxes\"] = boxes\n","                    target[\"labels\"] = labels\n","                    target[\"area\"] = area\n","\n","                    image_id = torch.tensor([idx])\n","                    target[\"image_id\"] = image_id\n","                    \n","                    # aply transformations of bounding box \n","                    if self.transforms:\n","                        sample = self.transforms(image = image_resized,\n","                                                bboxes = target['boxes'],\n","                                                labels = labels)\n","                        image_resized = sample['image']\n","                        target['boxes'] = torch.Tensor(sample['bboxes'])\n","            \n","            return image_resized, target     \n","    \n","    def __len__(self):\n","        return len(self.all_images)\n","    \n","# initialize the Dataset class and the dataloader\n","    \n","train_dataset = KDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n","valid_dataset = KDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=collate_fn\n",")\n","valid_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=collate_fn\n",")\n","print(f\"Number of training samples: {len(train_dataset)}\")\n","print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n","\n","\n","# visualize samnples to check if data prepoeration worked fine\n","if __name__ == '__main__':\n","\n","    dataset = KDataset(\n","        TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n","    )\n","    print(f\"Number of training images: {len(dataset)}\")\n","    \n","    def visualize_sample(image, target):\n","        box = target['boxes'][0]\n","        label = \"Allen_Bradley\"\n","        cv2.rectangle(\n","            image, \n","            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n","            (0, 255, 0), 1\n","        )\n","        cv2.putText(\n","            image, label, (int(box[0]), int(box[1]-5)), \n","            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n","        )\n","        plt.imshow(image)        \n","        plt.show()\n","        cv2.waitKey(0)\n","        \n","    NUM_SAMPLES_TO_VISUALIZE = 1\n","    for i in range(NUM_SAMPLES_TO_VISUALIZE):\n","        image, target = dataset[i]\n","        visualize_sample(image, target)"],"metadata":{"id":"up1E_0E0__Yj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"XtyF6i6NDEgn"}},{"cell_type":"code","source":["def create_model(num_classes):\n","    # load the model \n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    \n","    # change tho model to the input features of the localization task (2)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features \n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n","    \n","    return  model\n","    \n"],"metadata":{"id":"6sa1rzRdDFyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"kw6BIcFrDSW0"}},{"cell_type":"code","source":["plt.style.use('ggplot')\n","\n","\n","def train(train_data_loader, model):\n","    print('Training')\n","    global train_itr\n","    global train_loss_list\n","    \n","     # initialize tqdm progress bar\n","    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n","    for i, data in enumerate(prog_bar):\n","        optimizer.zero_grad()\n","        images, targets = data\n","        \n","        images = list(image.to(DEVICE) for image in images)\n","        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        loss_value = losses.item()\n","        train_loss_list.append(loss_value)\n","        train_loss_hist.send(loss_value)\n","        losses.backward()\n","        optimizer.step()\n","        train_itr += 1\n","    \n","        # update the loss value beside the progress bar for each iteration\n","        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n","    return train_loss_list\n","\n","\n","def validate(valid_data_loader, model):\n","    print('Validating')\n","    global val_itr\n","    global val_loss_list\n","    \n","    # initialize tqdm progress bar\n","    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n","    \n","    for i, data in enumerate(prog_bar):\n","        images, targets = data\n","        \n","        images = list(image.to(DEVICE) for image in images)\n","        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","        \n","        with torch.no_grad():\n","            loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        loss_value = losses.item()\n","        val_loss_list.append(loss_value)\n","        val_loss_hist.send(loss_value)\n","        val_itr += 1\n","        # update the loss value beside the progress bar for each iteration\n","        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n","    return val_loss_list\n","\n","\n","if __name__ == '__main__':\n","    # initialize the model and move to the computation device\n","    model = create_model(num_classes=NUM_CLASSES)\n","    model = model.to(DEVICE)\n","    # get the model parameters\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    # define the optimizer\n","    optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","    # initialize the Averager class\n","    train_loss_hist = Averager()\n","    val_loss_hist = Averager()\n","    train_itr = 1\n","    val_itr = 1\n","    # train and validation loss lists \n","    train_loss_list = []\n","    val_loss_list = []\n","    # name to save the trained model with\n","    MODEL_NAME = 'model'\n","    # whether to show transformed images from data loader or not\n","    if VISUALIZE_TRANSFORMED_IMAGES:\n","        show_transformed_image(train_loader)\n","    # start the training epochs\n","    for epoch in range(NUM_EPOCHS):\n","        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n","        # reset the training and validation loss histories for the current epoch\n","        train_loss_hist.reset()\n","        val_loss_hist.reset()\n","        # create two subplots, one for each, training and validation\n","        figure_1, train_ax = plt.subplots()\n","        figure_2, valid_ax = plt.subplots()\n","        # start timer and carry out training and validation\n","        start = time.time()\n","        train_loss = train(train_loader, model)\n","        val_loss = validate(valid_loader, model)\n","        print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")   \n","        print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\")   \n","        end = time.time()\n","        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n","        if (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n","            torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n","            print('SAVING MODEL COMPLETE...\\n')\n","        \n","        if (epoch+1) % SAVE_PLOTS_EPOCH == 0: # save loss plots after n epochs\n","            train_ax.plot(train_loss, color='blue')\n","            train_ax.set_xlabel('iterations')\n","            train_ax.set_ylabel('train loss')\n","            valid_ax.plot(val_loss, color='red')\n","            valid_ax.set_xlabel('iterations')\n","            valid_ax.set_ylabel('validation loss')\n","            figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n","            figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n","            print('SAVING PLOTS COMPLETE...')\n","        \n","        if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n","            train_ax.plot(train_loss, color='blue')\n","            train_ax.set_xlabel('iterations')\n","            train_ax.set_ylabel('train loss')\n","            valid_ax.plot(val_loss, color='red')\n","            valid_ax.set_xlabel('iterations')\n","            valid_ax.set_ylabel('validation loss')\n","            figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n","            figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n","            torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n","        \n","        plt.close('all')"],"metadata":{"id":"RRhi2Iv6DTtx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Eval"],"metadata":{"id":"zFSwBk2qE3L5"}},{"cell_type":"code","source":["\n","\n","\n","device = torch.device('cuda')\n","\n","# load model and weights \n","model = create_model(num_classes=2).to(device)\n","model.load_state_dict(torch.load(\n","    '/content/drive/MyDrive/DeepVis/Kontron/Newdata/output_test/model50.pth', map_location=device\n","))\n","# put model into evaluation mode\n","model.eval()\n","\n","# load test data \n","DIR_TEST = '/content/drive/MyDrive/DeepVis/Kontron/Newdata/test'\n","test_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\n","bb_test = glob.glob(f\"{DIR_TEST}/*.xml\")\n","\n","print(test_images)\n","print(f\"Test instances: {len(test_images)}\")\n","\n","# define classes\n","CLASSES = ['background', 'Allen-Bradley']\n","\n","# set confidence threshold for bounding box \n","detection_threshold = 0.9\n","# prepare lists to measure accuracy of the mdoel \n","\n","bb_pred_list = [] \n","bb_truth_list = []\n","IOU = []\n","\n","# predict bb for test images \n","for i in range(len(test_images)):\n","    image_name = test_images[i].split('/')[-1].split('.')[0]\n","    print(test_images[i])\n","    print(f\"ImagePath: {image_name}\")\n","    image = cv2.imread(test_images[i])\n","    orig_image = image.copy()\n","\n","  # adjust color and perform quick normalization\n","    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    image /= 255.0\n","\n","    image = np.transpose(image, (2, 0, 1)).astype(np.float)\n","\n","    image = torch.tensor(image, dtype=torch.float).cuda()\n","\n","    image = torch.unsqueeze(image, 0)\n","\n","    # predict bb\n","    with torch.no_grad():\n","        outputs = model(image)\n","    \n","    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n","    \n","    # get truth bb out of xml file \n","    if len(outputs[0]['boxes']) != 0:\n","        bb_file_path = test_images[i][:-4] + '.xml'\n","        print(f\"XmlPath: {bb_file_path}\")\n","        tree = et.parse(bb_file_path)\n","        root = tree.getroot()\n","\n","        for member in root.findall('object'):\n","                if member.find('name').text == 'CamFront':\n","                    xmin = int(member.find('bndbox').find('xmin').text)\n","                    xmax = int(member.find('bndbox').find('xmax').text)\n","                    ymin = int(member.find('bndbox').find('ymin').text)\n","                    ymax = int(member.find('bndbox').find('ymax').text)\n","\n","                    bb_truth = [xmin,ymin,xmax,ymax]\n","\n","        bb_truth_list += bb_truth\n","\n","        bb_pred_tensor = outputs[0]['boxes'][0]\n","        bb_pred_list += torch.Tensor.tolist(bb_pred_tensor)\n","        bb_pred_tensor = bb_pred_tensor.unsqueeze(0)\n","\n","        bb_truth_tensor = torch.tensor(bb_truth, dtype=torch.float).unsqueeze(0)\n","        # display coordinates of predicted and truth bb and calculate IoU\n","        print(f\"Truth: {bb_truth_tensor}\")\n","        print(f\"Pred: {bb_pred_tensor}\")\n","        iou = torchvision.ops.box_iou(bb_truth_tensor, bb_pred_tensor)\n","        IOU.append(iou)\n","        print(f\"img {i} IoU: {iou}\")\n","\n","        boxes = outputs[0]['boxes'].data.numpy()\n","        scores = outputs[0]['scores'].data.numpy()\n","\n","        # only keep box prediction with high confidence \n","        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n","        print(f\"scores: {scores}\")\n","   # draw the bounding boxes and write the class name on top of it\n","        draw_boxes = boxes.copy()\n","\n","        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n","        \n","       \n","        print(bb_truth)\n","        for j, box in enumerate(draw_boxes):\n","            cv2.rectangle(orig_image,\n","                        (int(box[0]), int(box[1])),\n","                        (int(box[2]), int(box[3])),\n","                        (0, 255, 0), 4)\n","            \n","            cv2.rectangle(orig_image,\n","                        (int(bb_truth[0]), int(bb_truth[1])),\n","                        (int(bb_truth[2]), int(bb_truth[3])),\n","                        (255, 0, 0), 2)\n","                       \n","            cv2.putText(orig_image, pred_classes[j], \n","                        (int(box[0]), int(box[1]-5)),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n","                        2, lineType=cv2.LINE_AA)\n","        plt.imshow(orig_image)\n","        plt.show()\n","        cv2.waitKey(1)\n","        print(int(box[0]),box[1],box[2],box[3])\n","        img_cropped = orig_image[int(box[1]):int(box[3]),int(box[0]):int(box[2])]\n","        plt.imshow(img_cropped)\n","        plt.show()\n","        cv2.imwrite(f\"/content/drive/MyDrive/DeepVis/Kontron/Newdata/output_test/{image_name}.jpg\", img_cropped)\n","        #cv2.imwrite(f\"/content/drive/MyDrive/DeepVis/Kontron/Newdata/output_test/{image_name}.jpg\", orig_image)\n","\n","    print(f\"Image {i+1} done...\")\n","    print('-'*50)\n","print('TEST PREDICTIONS COMPLETE')\n","cv2.destroyAllWindows()\n","print(\"\")\n","# display IoUs and avg IoU\n","IOU_f = [float(a.item()) for a in IOU]\n","print(f\"IOUs: {IOU_f}\")\n","avgIOU = sum(IOU_f) / len(IOU_f)\n","print(f\"avg IOU: {avgIOU}\")\n","\n","# calculate percision and recall\n","y_true = []\n","for i in range(len(test_images)):\n","  y_true.append(\"positive\")\n","threshold = 0.95\n","y_pred = [\"positive\" if score >= threshold else \"negative\" for score in IOU_f]\n","print(f\"true: {y_true}\")\n","print(f\"pred: {y_pred}\")\n","\n","\n","\n","\n","\n","precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\n","print(f\"percision: {precision}\")\n","\n","recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\")\n","print(f\"recall: {recall}\")"],"metadata":{"id":"KGZOX40fE2j-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"SVKOOOUmizHZ"}},{"cell_type":"markdown","source":["Citation:\n","https://debuggercafe.com/custom-object-detection-using-pytorch-faster-rcnn/"],"metadata":{"id":"YeUoGnvki5b6"}}]}